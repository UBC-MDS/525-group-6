{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 525 Lab 1 \n",
    " \n",
    "**Group 6: Crystal Geng, Tony Zoght, Chester Wang, HanChen Wang**\n",
    "\n",
    "\n",
    "In this project, our objective is to get familiar with processing datasets and constructing machine learning models at scale. We will be working with a dataset containing daily precipitation data from New South Wales (NSW), Australia, covering the years from 1889 to 2014. The dataset is derived from CMIP6, a global cooperative initiative that brings together climate modeling outputs from various research groups.\n",
    "\n",
    "The dataset comprises 6 columns, which represent timestamps, spatial data, and rainfall amounts (in mm/day). Our initial task (milestone 1) involves consolidating the modelled datasets into a single CSV file to facilitate subsequent processing. As the project progresses, we anticipate developing and deploying ensemble machine learning models in the cloud to forecast daily rainfall in Australia based on the dataset. The dataset features outputs from multiple climate models, with the actual rainfall observation as the target variable.\n",
    "\n",
    "We have sourced the dataset from figshare, and our ultimate aim is to develop an ensemble model that leverages these outputs and compare its predictions with the actual rainfall data. By the end of the project, we aim to have a cloud-deployed machine learning model available for public use."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Team-work contract\n",
    "rubric={correctness:10}\n",
    "\n",
    "Similar to what you did in DSCI 522 and DSCI 524, create a teamwork contract. The contract should outline how you are committed to working together so that you are accountable to one another. Again, you may start with your team contract document from previous project courses and adapt it to your new team. It is a fairly personal document, and please do not push it into your public repositories. Instead, save it somewhere your team can easily share it, and you can share a link to it or a copy with us in your submission to Canvas to prove you did this."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The teamwork contract link will be appended in the Canvas Submission. Since it's a rather private documentation, we will not be including it here. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating a repository and project structure \n",
    "rubric={mechanics:10}\n",
    "\n",
    "1. Similar to previous project courses, create a public repository under [UBC-MDS org](https://github.com/UBC-MDS) for your project. \n",
    "2. Write a brief introduction of the project in the `README`. \n",
    "3. Create a folder called `notebooks` in the repository and create a notebook for this milestone in that folder.\n",
    "\n",
    "\n",
    "The project GitHub repo can be accessed using the link: https://github.com/UBC-MDS/525-group-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages for analysis\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.request import urlretrieve\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Downloading the data \n",
    "rubric={correctness:10}\n",
    "\n",
    "1. Download the data from [figshare](https://figshare.com/articles/dataset/Daily_rainfall_over_NSW_Australia/14096681) to your local computer using the [figshare API](https://docs.figshare.com) (you need to make use of `requests` library).\n",
    "\n",
    "2. Extract the zip file, again programmatically, similar to how we did it in class. \n",
    "\n",
    ">  You can download the data and unzip it manually. But we learned about APIs, so we can do it in a reproducible way with the `requests` library, similar to how we [did it in class](https://pages.github.ubc.ca/MDS-2022-23/DSCI_525_web-cloud-comp_students/lectures/lecture1.html#using-rest-api-lab-lecture).\n",
    "\n",
    "> There are 5 files in the figshare repo. The one we want is: `data.zip`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nagivating to the location folder that you want to download the files to. Here we are using HanChen's notebook as an example. The code is adapted from the DSCI 525 lecture notes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: /Users/hcwang/Documents/GitHub/Block6/figshareexp: File exists\n",
      "/Users/hcwang/Documents/GitHub/Block6/figshareexp\n"
     ]
    }
   ],
   "source": [
    "## Change it to the location that you want to download your files to.\n",
    "%mkdir /Users/hcwang/Documents/GitHub/Block6/figshareexp\n",
    "%cd /Users/hcwang/Documents/GitHub/Block6/figshareexp\n",
    "\n",
    "# Necessary metadata\n",
    "article_id = 14096681  # this is the unique identifier of the article on figshare\n",
    "url = f\"https://api.figshare.com/v2/articles/{article_id}\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "output_directory = \"figsharerainfall/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are sending a `GET` request to list the available files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 26579150,\n",
       "  'name': 'daily_rainfall_2014.png',\n",
       "  'size': 58863,\n",
       "  'is_link_only': False,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26579150',\n",
       "  'supplied_md5': 'fd32a2ffde300a31f8d63b1825d47e5e',\n",
       "  'computed_md5': 'fd32a2ffde300a31f8d63b1825d47e5e'},\n",
       " {'id': 26579171,\n",
       "  'name': 'environment.yml',\n",
       "  'size': 192,\n",
       "  'is_link_only': False,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26579171',\n",
       "  'supplied_md5': '060b2020017eed93a1ee7dd8c65b2f34',\n",
       "  'computed_md5': '060b2020017eed93a1ee7dd8c65b2f34'},\n",
       " {'id': 26586554,\n",
       "  'name': 'README.md',\n",
       "  'size': 5422,\n",
       "  'is_link_only': False,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26586554',\n",
       "  'supplied_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c',\n",
       "  'computed_md5': '61858c6cc0e6a6d6663a7e4c75bbd88c'},\n",
       " {'id': 26766812,\n",
       "  'name': 'data.zip',\n",
       "  'size': 814041183,\n",
       "  'is_link_only': False,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26766812',\n",
       "  'supplied_md5': 'b517383f76e77bd03755a63a8ff83ee9',\n",
       "  'computed_md5': 'b517383f76e77bd03755a63a8ff83ee9'},\n",
       " {'id': 26766815,\n",
       "  'name': 'get_data.py',\n",
       "  'size': 4113,\n",
       "  'is_link_only': False,\n",
       "  'download_url': 'https://ndownloader.figshare.com/files/26766815',\n",
       "  'supplied_md5': '7829028495fd9dec9680ea013474afa6',\n",
       "  'computed_md5': '7829028495fd9dec9680ea013474afa6'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = requests.request(\"GET\", url, headers=headers)\n",
    "data = json.loads(response.text)  # this contains all the articles data, feel free to check it out\n",
    "files = data[\"files\"]             # this is just the data about the files, which is what we want\n",
    "files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we download the specific data file we will be using in this exercise. Uncomment the code below to begin downloading. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# files_to_dl = [\"data.zip\"]  # feel free to add other files here\n",
    "# for file in files:\n",
    "#     if file[\"name\"] in files_to_dl:\n",
    "#         os.makedirs(output_directory, exist_ok=True)\n",
    "#         urlretrieve(file[\"download_url\"], output_directory + file[\"name\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the code below to begin unzipping the downloaded zipped file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# with zipfile.ZipFile(os.path.join(output_directory, \"data.zip\"), 'r') as f:\n",
    "#     f.extractall(output_directory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a list of available files in the local destination folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hcwang/Documents/GitHub/Block6/figshareexp/figsharerainfall\n",
      "total 22326064\n",
      "-rw-r--r--   1 hcwang  staff    95376895 Mar 27 14:28 MPI-ESM-1-2-HAM_daily_rainfall_NSW.csv\n",
      "-rw-r--r--   1 hcwang  staff    94960113 Mar 27 14:28 AWI-ESM-1-1-LR_daily_rainfall_NSW.csv\n",
      "-rw-r--r--   1 hcwang  staff    82474546 Mar 27 14:28 NorESM2-LM_daily_rainfall_NSW.csv\n",
      "-rw-r--r--   1 hcwang  staff   127613760 Mar 27 14:28 ACCESS-CM2_daily_rainfall_NSW.csv\n",
      "-rw-r--r--   1 hcwang  staff   232118894 Mar 27 14:28 FGOALS-f3-L_daily_rainfall_NSW.csv\n",
      "-rw-r--r--   1 hcwang  staff   330360682 Mar 27 14:28 CMCC-CM2-HR4_daily_rainfall_NSW.csv\n",
      "-rw-r--r--   1 hcwang  staff   254009247 Mar 27 14:28 MRI-ESM2-0_daily_rainfall_NSW.csv\n",
      "-rw-r--r--   1 hcwang  staff   235661418 Mar 27 14:28 GFDL-CM4_daily_rainfall_NSW.csv\n",
      "-rw-r--r--   1 hcwang  staff   294260911 Mar 27 14:28 BCC-CSM2-MR_daily_rainfall_NSW.csv\n",
      "-rw-r--r--   1 hcwang  staff   295768615 Mar 27 14:28 EC-Earth3-Veg-LR_daily_rainfall_NSW.csv\n",
      "-rw-r--r--   1 hcwang  staff   328852379 Mar 27 14:28 CMCC-ESM2_daily_rainfall_NSW.csv\n",
      "-rw-r--r--   1 hcwang  staff    67784105 Mar 27 14:28 NESM3_daily_rainfall_NSW.csv\n",
      "-rw-r--r--   1 hcwang  staff    95640682 Mar 27 14:28 MPI-ESM1-2-LR_daily_rainfall_NSW.csv\n",
      "-rw-r--r--   1 hcwang  staff   114707410 Mar 27 14:28 ACCESS-ESM1-5_daily_rainfall_NSW.csv\n",
      "-rw-r--r--   1 hcwang  staff   116179272 Mar 27 14:28 FGOALS-g3_daily_rainfall_NSW.csv\n",
      "-rw-r--r--   1 hcwang  staff   102517965 Mar 27 14:28 INM-CM4-8_daily_rainfall_NSW.csv\n",
      "-rw-r--r--   1 hcwang  staff   515458033 Mar 27 14:28 MPI-ESM1-2-HR_daily_rainfall_NSW.csv\n",
      "-rw-r--r--   1 hcwang  staff   332813281 Mar 27 14:28 TaiESM1_daily_rainfall_NSW.csv\n",
      "-rw-r--r--   1 hcwang  staff   337555851 Mar 27 14:28 NorESM2-MM_daily_rainfall_NSW.csv\n",
      "-rw-r--r--   1 hcwang  staff   328787320 Mar 27 14:28 CMCC-CM2-SR5_daily_rainfall_NSW.csv\n",
      "-rw-r--r--   1 hcwang  staff      952202 Mar 27 14:28 observed_daily_rainfall_SYD.csv\n",
      "-rw-r--r--   1 hcwang  staff    93829697 Mar 27 14:28 KIOST-ESM_daily_rainfall_NSW.csv\n",
      "-rw-r--r--   1 hcwang  staff   102692289 Mar 27 14:28 INM-CM5-0_daily_rainfall_NSW.csv\n",
      "-rw-r--r--   1 hcwang  staff   206822938 Mar 27 14:28 MIROC6_daily_rainfall_NSW.csv\n",
      "-rw-r--r--   1 hcwang  staff    55224437 Mar 27 14:28 BCC-ESM1_daily_rainfall_NSW.csv\n",
      "-rw-r--r--   1 hcwang  staff   124586961 Mar 27 14:28 GFDL-ESM4_daily_rainfall_NSW.csv\n",
      "-rw-r--r--   1 hcwang  staff    46286371 Mar 27 14:28 CanESM5_daily_rainfall_NSW.csv\n",
      "-rw-r--r--   1 hcwang  staff   333489879 Mar 27 14:28 SAM0-UNICON_daily_rainfall_NSW.csv\n",
      "drwxr-xr-x  30 hcwang  staff         960 Mar 27 14:28 \u001b[1m\u001b[34m__MACOSX\u001b[m\u001b[m/\n",
      "-rw-r--r--   1 hcwang  staff    77848576 Mar 27 14:38 data.zip\n",
      "-rw-r--r--   1 hcwang  staff  6004371870 Mar 27 15:10 daily_rainfall.csv\n"
     ]
    }
   ],
   "source": [
    "%cd figsharerainfall/\n",
    "%ls -ltr "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Combining data CSVs\n",
    "rubric={correctness:10,reasoning:10}\n",
    "\n",
    "1. Combine data CSVs into a single CSV using pandas.\n",
    "    \n",
    "2. When combining the CSV files, add an extra column called \"model\" that identifies the model.\n",
    "    Tip 1: you can get this column populated from the file name, eg: for file name \"SAM0-UNICON_daily_rainfall_NSW.csv\", the model name is SAM0-UNICON\n",
    "    Tip 2: Remember how we added \"year\" column when we combined airline CSVs. Here the regex will be to get word before an underscore ie, \"/([^_]*)\"\n",
    "\n",
    "> Note: There is a file called `observed_daily_rainfall_SYD.csv` in the data folder that you downloaded. Make sure you exclude this file (programmatically or just take out that file from the folder) before you combine CSVs. We will use this file in our next milestone.\n",
    "\n",
    "3. ***Compare*** run times on different machines within your team and summarize your observations. \n",
    "\n",
    "> Warning: Some of you might not be able to do it on your laptop. It's fine if you're unable to do it. Just make sure you discuss the reasons why you might not have been able to run this on your laptop. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are iterating through all csv files in the local directory, excluding the `observed_daily_rainfall_SYD.csv` file and the output file `daily_rainfall.csv`.  \n",
    "\n",
    "We checked that the csvs have the same number of columns so we can simply concatenate the dataframes. The merged dataframe `daily_rainfall_df` will also include a column indicating its source (`model` column). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPI-ESM-1-2-HAM_daily_rainfall_NSW.csv\n",
      "AWI-ESM-1-1-LR_daily_rainfall_NSW.csv\n",
      "NorESM2-LM_daily_rainfall_NSW.csv\n",
      "ACCESS-CM2_daily_rainfall_NSW.csv\n",
      "FGOALS-f3-L_daily_rainfall_NSW.csv\n",
      "CMCC-CM2-HR4_daily_rainfall_NSW.csv\n",
      "MRI-ESM2-0_daily_rainfall_NSW.csv\n",
      "GFDL-CM4_daily_rainfall_NSW.csv\n",
      "BCC-CSM2-MR_daily_rainfall_NSW.csv\n",
      "EC-Earth3-Veg-LR_daily_rainfall_NSW.csv\n",
      "CMCC-ESM2_daily_rainfall_NSW.csv\n",
      "NESM3_daily_rainfall_NSW.csv\n",
      "MPI-ESM1-2-LR_daily_rainfall_NSW.csv\n",
      "ACCESS-ESM1-5_daily_rainfall_NSW.csv\n",
      "FGOALS-g3_daily_rainfall_NSW.csv\n",
      "INM-CM4-8_daily_rainfall_NSW.csv\n",
      "MPI-ESM1-2-HR_daily_rainfall_NSW.csv\n",
      "TaiESM1_daily_rainfall_NSW.csv\n",
      "NorESM2-MM_daily_rainfall_NSW.csv\n",
      "CMCC-CM2-SR5_daily_rainfall_NSW.csv\n",
      "KIOST-ESM_daily_rainfall_NSW.csv\n",
      "INM-CM5-0_daily_rainfall_NSW.csv\n",
      "MIROC6_daily_rainfall_NSW.csv\n",
      "BCC-ESM1_daily_rainfall_NSW.csv\n",
      "GFDL-ESM4_daily_rainfall_NSW.csv\n",
      "CanESM5_daily_rainfall_NSW.csv\n",
      "SAM0-UNICON_daily_rainfall_NSW.csv\n",
      "CPU times: user 32.1 s, sys: 25.8 s, total: 57.9 s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "daily_rainfall_df = pd.DataFrame()\n",
    "for file in os.listdir(\".\"):\n",
    "    if file[-3:] == 'csv' and file != 'observed_daily_rainfall_SYD.csv' and file != 'daily_rainfall.csv':\n",
    "        print(file)\n",
    "        data = pd.read_csv(file)\n",
    "        data['model'] = file.split(\"_\")[0]\n",
    "        daily_rainfall_df = pd.concat([daily_rainfall_df, data])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the merged dataframe and confirmed with lab members that the row and column numbers are correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62467843, 7)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_rainfall_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing the merged dataframe to csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 55s, sys: 7.02 s, total: 3min 2s\n",
      "Wall time: 3min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "daily_rainfall_df.to_csv(\"daily_rainfall.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3747499371 6004371870 daily_rainfall.csv\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "cksum daily_rainfall.csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Compare run times on different machines within your team and summarize your observations.\n",
    "| Team Member | Operating System | RAM | Processor | Is SSD | Time to merge dataset| Time to write to a new csv|\n",
    "|:-----------:|:----------------:|:---:|:---------:|:------:|:----------:|:--:|\n",
    "| HanChen Wang| OSX 13.2.1 | 8GB |Apple M2 (8 Cores)|Yes| 1m6s |3m3s|\n",
    "| Tony Zoght  | OSX 12.5   | 32GB  |  Apple M1 (10 Cores         | Yes |    ||\n",
    "| Chester Wang    | OSX 12.5                 |8GB     |2.9 GHz Dual-Core Intel Core i5           |Yes       |            ||\n",
    "| Crystal Geng   |  OSX 12.6      | 8GB    | Apple M2 (8 Cores)       |   Yes     |     29.5s       | 2min45s|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`[NEED TO write more description on this]`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load the combined CSV to memory and perform a simple EDA\n",
    "rubric={correctness:10,reasoning:10}\n",
    "\n",
    "1. Investigate at least two of the following approaches to reduce memory usage while performing the EDA (e.g., value_counts). Refer to lecture notes [here](https://pages.github.ubc.ca/MDS-2022-23/DSCI_525_web-cloud-comp_students/lectures/lecture1.html#some-tactics-to-deal-with-memory-issue).\n",
    "    - Changing `dtype` of your data\n",
    "    - Load just columns that we want\n",
    "    - Loading in chunks\n",
    "    \n",
    "2. ***Compare*** run times on different machines within your team and summarize your observations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are performed the data loading using three techniques discussed in the DSCI 525 Lecture 1, which are:\n",
    "1. Loading the entire table all at once (the slowest method).  \n",
    "2. Loading the specific column of interest only. \n",
    "3. Laoding the csv in chunks. \n",
    "\n",
    "We will then perform a simple EDA on the number of observations for each model (`.value_counts` on the `model` column).\n",
    "\n",
    "Before starting with the data loading, We first measured the speed to perform the simply EDA using preloaded dataframe above. This shows that the simple EDA step `does not` take a significant amount of time to complete (~ 1-2 seconds). Thus, we can use this to calculate the time each method takes to load the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.14 s, sys: 85.2 ms, total: 1.22 s\n",
      "Wall time: 1.28 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MPI-ESM1-2-HR       5154240\n",
       "CMCC-CM2-HR4        3541230\n",
       "CMCC-ESM2           3541230\n",
       "CMCC-CM2-SR5        3541230\n",
       "NorESM2-MM          3541230\n",
       "TaiESM1             3541230\n",
       "SAM0-UNICON         3541153\n",
       "GFDL-ESM4           3219300\n",
       "FGOALS-f3-L         3219300\n",
       "GFDL-CM4            3219300\n",
       "MRI-ESM2-0          3037320\n",
       "EC-Earth3-Veg-LR    3037320\n",
       "BCC-CSM2-MR         3035340\n",
       "MIROC6              2070900\n",
       "ACCESS-CM2          1932840\n",
       "ACCESS-ESM1-5       1610700\n",
       "INM-CM4-8           1609650\n",
       "INM-CM5-0           1609650\n",
       "FGOALS-g3           1287720\n",
       "KIOST-ESM           1287720\n",
       "AWI-ESM-1-1-LR       966420\n",
       "MPI-ESM1-2-LR        966420\n",
       "NESM3                966420\n",
       "MPI-ESM-1-2-HAM      966420\n",
       "NorESM2-LM           919800\n",
       "BCC-ESM1             551880\n",
       "CanESM5              551880\n",
       "Name: model, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "daily_rainfall_df['model'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Loading the entire table all at once (the slowest method).  \n",
    "This method takes from 30-106 seconds to complete depending on the computer we have. We will use this as the reference baseline when comparing with the other methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPI-ESM1-2-HR       5154240\n",
      "CMCC-CM2-HR4        3541230\n",
      "CMCC-ESM2           3541230\n",
      "CMCC-CM2-SR5        3541230\n",
      "NorESM2-MM          3541230\n",
      "TaiESM1             3541230\n",
      "SAM0-UNICON         3541153\n",
      "GFDL-ESM4           3219300\n",
      "FGOALS-f3-L         3219300\n",
      "GFDL-CM4            3219300\n",
      "MRI-ESM2-0          3037320\n",
      "EC-Earth3-Veg-LR    3037320\n",
      "BCC-CSM2-MR         3035340\n",
      "MIROC6              2070900\n",
      "ACCESS-CM2          1932840\n",
      "ACCESS-ESM1-5       1610700\n",
      "INM-CM4-8           1609650\n",
      "INM-CM5-0           1609650\n",
      "FGOALS-g3           1287720\n",
      "KIOST-ESM           1287720\n",
      "AWI-ESM-1-1-LR       966420\n",
      "MPI-ESM1-2-LR        966420\n",
      "NESM3                966420\n",
      "MPI-ESM-1-2-HAM      966420\n",
      "NorESM2-LM           919800\n",
      "BCC-ESM1             551880\n",
      "CanESM5              551880\n",
      "Name: model, dtype: int64\n",
      "CPU times: user 27.8 s, sys: 6.29 s, total: 34 s\n",
      "Wall time: 36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Reading all at once\n",
    "df = pd.read_csv(\"daily_rainfall.csv\")\n",
    "print(df[\"model\"].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Loading the specific column of interest only.  \n",
    "This method takes less than half of the time compared to 5.1 on all our computers. This can be attributed to us pre-defining the column to read and thus it requires less time and memory to load the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPI-ESM1-2-HR       5154240\n",
      "CMCC-CM2-HR4        3541230\n",
      "CMCC-ESM2           3541230\n",
      "CMCC-CM2-SR5        3541230\n",
      "NorESM2-MM          3541230\n",
      "TaiESM1             3541230\n",
      "SAM0-UNICON         3541153\n",
      "GFDL-ESM4           3219300\n",
      "FGOALS-f3-L         3219300\n",
      "GFDL-CM4            3219300\n",
      "MRI-ESM2-0          3037320\n",
      "EC-Earth3-Veg-LR    3037320\n",
      "BCC-CSM2-MR         3035340\n",
      "MIROC6              2070900\n",
      "ACCESS-CM2          1932840\n",
      "ACCESS-ESM1-5       1610700\n",
      "INM-CM4-8           1609650\n",
      "INM-CM5-0           1609650\n",
      "FGOALS-g3           1287720\n",
      "KIOST-ESM           1287720\n",
      "AWI-ESM-1-1-LR       966420\n",
      "MPI-ESM1-2-LR        966420\n",
      "NESM3                966420\n",
      "MPI-ESM-1-2-HAM      966420\n",
      "NorESM2-LM           919800\n",
      "BCC-ESM1             551880\n",
      "CanESM5              551880\n",
      "Name: model, dtype: int64\n",
      "CPU times: user 15.4 s, sys: 1.52 s, total: 17 s\n",
      "Wall time: 17.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Reading specific column only\n",
    "use_cols = ['model']\n",
    "df = pd.read_csv(\"daily_rainfall.csv\",usecols=use_cols)\n",
    "print(df[\"model\"].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Laoding the csv in chunks. \n",
    "This method takes slightly less time than the method in 5.1 on all our computers. However, we have the better method in 5.2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCESS-CM2          1932840\n",
      "ACCESS-ESM1-5       1610700\n",
      "AWI-ESM-1-1-LR       966420\n",
      "BCC-CSM2-MR         3035340\n",
      "BCC-ESM1             551880\n",
      "CMCC-CM2-HR4        3541230\n",
      "CMCC-CM2-SR5        3541230\n",
      "CMCC-ESM2           3541230\n",
      "CanESM5              551880\n",
      "EC-Earth3-Veg-LR    3037320\n",
      "FGOALS-f3-L         3219300\n",
      "FGOALS-g3           1287720\n",
      "GFDL-CM4            3219300\n",
      "GFDL-ESM4           3219300\n",
      "INM-CM4-8           1609650\n",
      "INM-CM5-0           1609650\n",
      "KIOST-ESM           1287720\n",
      "MIROC6              2070900\n",
      "MPI-ESM-1-2-HAM      966420\n",
      "MPI-ESM1-2-HR       5154240\n",
      "MPI-ESM1-2-LR        966420\n",
      "MRI-ESM2-0          3037320\n",
      "NESM3                966420\n",
      "NorESM2-LM           919800\n",
      "NorESM2-MM          3541230\n",
      "SAM0-UNICON         3541153\n",
      "TaiESM1             3541230\n",
      "dtype: int64\n",
      "CPU times: user 28.1 s, sys: 3.96 s, total: 32 s\n",
      "Wall time: 22min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Reading in chucks\n",
    "counts = pd.Series(dtype=int)\n",
    "for chunk in pd.read_csv(\"daily_rainfall.csv\", chunksize=10_000_000):\n",
    "    counts = counts.add(chunk[\"model\"].value_counts(), fill_value=0)\n",
    "print(counts.astype(int))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Compare run times on different machines within your team and summarize your observations.\n",
    "From the comparison table below, it can be seen that the time taken to read data (for all three tasks) are fairly consistent between Apple M1 and Apple M2 computers, with the speed in all three tasks slightly faster with Tony's computer primarily due to the 32GB RAM and 10 Cores processor. The 2.9GHz Dual-Core Intel Core i5 took the longest time for all three tasks. \n",
    "\n",
    "`please add more`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra: storing the dataframe with different dataTypes  \n",
    "\n",
    "We experimented storing the loaded dataframe in `float64` and `float32` and it's apparent the `float32` format conserves memory usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage with float64: 3997.94 MB\n",
      "Memory usage with float32: 2748.59 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Memory usage with float64: {daily_rainfall_df.memory_usage().sum() / 1e6:.2f} MB\")\n",
    "print(f\"Memory usage with float32: {daily_rainfall_df.astype('float32', errors='ignore').memory_usage().sum() / 1e6:.2f} MB\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Team Member | Operating System | RAM | Processor | Is SSD | Time taken to read all at once| Time taken to read model column only| Time taken to read in chunks|\n",
    "|:-----------:|:----------------:|:---:|:---------:|:------:|:----------:|:----------:|:----------:|\n",
    "| HanChen Wang| OSX 13.2.1 | 8GB |Apple M2 (8 Cores)|Yes| 37.1s|17.6s|33.6s|\n",
    "| Tony Zoght  | OSX 12.5   | 32GB  |  Apple M1 (10 Cores)         | Yes |  30.1s  |   14.3s    |   29.5s  |\n",
    "| Chester Wang    | OSX 12.5                 |8GB     |2.9 GHz Dual-Core Intel Core i5           |Yes       |109s            |48.2s            |94s            |\n",
    "| Crystal Geng   |  OSX 12.6      | 8GB    | Apple M2 (8 Cores)       |   Yes     |  36s          | 15.8s           |   33.4s   |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Perform a simple EDA in R\n",
    "rubric={correctness:15,reasoning:10}\n",
    "\n",
    "Choose one of the methods listed below for transferring the dataframe (i.e., the entire dataset) from Python to R, and explain why you opted for this approach instead of the others.\n",
    "- Parquet file\n",
    "- Pandas exchange\n",
    "- Arrow exchange\n",
    "\n",
    "Once you have the dataframe in R, perform a simple EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose to use Arrow for transferring the dataframe from Python to R because it shows enhanced performance and efficiency when we transfer between Python to R compared to Pandas exchange. This is primarily due to the fact that the time spent on serialization/deserialization process using Pandas is very long, while it only took 512 ms to finish the transferring using an arrow table. Although Parquet is another option that is more efficient than Pandas, Parquet requires additional on-disk storage since it would need to convert the csv file to Parquet files first. Therefore, using Arrow exchange might be more efficient and less time-consuming in our scenario since we are aiming for data exchange between Python and R. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.dataset as ds\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import pyarrow \n",
    "from pyarrow import csv\n",
    "import rpy2_arrow.pyarrow_rarrow as pyra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['R_HOME'] = '/Users/Jialing/opt/miniconda3/envs/525_2023/lib/R'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change it to the directory where your csv file is located \n",
    "filepathcsv = \"/Users/Jialing/525/lab1/combined_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.1 s, sys: 1.24 s, total: 12.4 s\n",
      "Wall time: 11.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset = ds.dataset(filepathcsv, format=\"csv\")\n",
    "\n",
    "table = dataset.to_table()\n",
    "\n",
    "r_table = pyra.converter.py2rpy(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# A tibble: 27 × 2\n",
      "   model                  n\n",
      "   <chr>              <int>\n",
      " 1 MPI-ESM-1-2-HAM   966420\n",
      " 2 AWI-ESM-1-1-LR    966420\n",
      " 3 NorESM2-LM        919800\n",
      " 4 ACCESS-CM2       1932840\n",
      " 5 FGOALS-f3-L      3219300\n",
      " 6 CMCC-CM2-HR4     3541230\n",
      " 7 MRI-ESM2-0       3037320\n",
      " 8 GFDL-CM4         3219300\n",
      " 9 BCC-CSM2-MR      3035340\n",
      "10 EC-Earth3-Veg-LR 3037320\n",
      "# ℹ 17 more rows\n",
      "# ℹ Use `print(n = ...)` to see more rows\n",
      "Time difference of 0.378994 secs\n",
      "# A tibble: 27 × 2\n",
      "   model                  n\n",
      "   <chr>              <int>\n",
      " 1 MPI-ESM-1-2-HAM   966420\n",
      " 2 AWI-ESM-1-1-LR    966420\n",
      " 3 NorESM2-LM        919800\n",
      " 4 ACCESS-CM2       1932840\n",
      " 5 FGOALS-f3-L      3219300\n",
      " 6 CMCC-CM2-HR4     3541230\n",
      " 7 MRI-ESM2-0       3037320\n",
      " 8 GFDL-CM4         3219300\n",
      " 9 BCC-CSM2-MR      3035340\n",
      "10 EC-Earth3-Veg-LR 3037320\n",
      "# ℹ 17 more rows\n",
      "# ℹ Use `print(n = ...)` to see more rows\n",
      "CPU times: user 1.22 s, sys: 575 ms, total: 1.79 s\n",
      "Wall time: 512 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R -i r_table\n",
    "start_time <- Sys.time()\n",
    "suppressMessages(library(dplyr))\n",
    "result <- r_table |> count(model) |> collect()\n",
    "end_time <- Sys.time()\n",
    "print(result %>% collect())\n",
    "print(end_time - start_time)\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('525_2023')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b66524be0b311ace9d9579045d9fd1b8b5099cec0f928d061b4f39f9dec3bd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
